# Retrieval-Augmented Generation (RAG) з LLM-as-a-Judge

Цей проєкт реалізує повний пайплайн для відповіді на запитання на основі датасету `SQuAD`-типу, з використанням Retrieval-Augmented Generation та оцінкою відповідей великою мовною моделлю (LLM-as-a-Judge).

## Структура пайплайну

### 1. Завантаження датасету
Використовується спрощений датасет `simplified_squad_300.json`, що містить:
- `question` — запитання
- `answer` — правильна відповідь
- `context` — текстовий фрагмент, що містить відповідь

> 🔸 Файл розміщується в `data/`

---

### 2. Створення бази знань
Контексти з усього датасету використовуються для побудови векторної бази знань за допомогою:
- `sentence-transformers/all-MiniLM-L6-v2`
- FAISS як векторне сховище

---

### 3. Генерація відповідей

- **Baseline**: відповідь генерується *лише на основі запитання*
- **RAG**: використовується retriever для знаходження релевантного контексту, який передається генеративній моделі

> 🔸 Генерація проводиться за допомогою `Ollama` (наприклад, `phi3`, `llama3`, тощо)

---

### 4. Оцінка якості відповідей

Оцінювання виконується LLM-моделлю через підхід **LLM-as-a-Judge**, яка отримує:
- питання
- правильну відповідь
- згенеровану відповідь

і видає оцінку від **1 до 5**, де:
- 1 = повністю неправильна
- 5 = ідеальна відповідь

> 🔸 Використовується модель `LLaMA 3 (70B)` через платформу Groq.

---

### 5. Оцінка Retrieval (Recall@K)

Для визначення якості відбору контекстів виконується оцінка:
- **Recall@10**
- **Recall@15**
- **Recall@20**
- **Recall@25**
- **Recall@30**

Метрика: чи потрапив правильний фрагмент контексту серед K найкращих.

---

## Вимоги

- Python ≥ 3.10
- Бібліотеки:
  - `langchain`
  - `langchain-community`
  - `sentence-transformers`
  - `faiss-cpu`
  - `groq` (або `openai`/`gemini`)
  - `ollama` (локальна ollama-модель)

> 🔸 Встановлення:
```bash
pip install -U langchain langchain-community sentence-transformers faiss-cpu groq ollama
